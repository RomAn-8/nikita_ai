"""LLM service wrapper for OpenRouter and Ollama."""

import requests
import logging
from typing import Any
from ..openrouter import chat_completion, chat_completion_raw
from ..config import (
    OPENROUTER_MODEL, OLLAMA_BASE_URL, OLLAMA_MODEL, OLLAMA_TIMEOUT,
    OLLAMA_TEMPERATURE, OLLAMA_NUM_CTX, OLLAMA_NUM_PREDICT, OLLAMA_SYSTEM_PROMPT,
    ANALYZE_MODEL
)

logger = logging.getLogger(__name__)


def call_llm(
    messages: list[dict[str, Any]],
    temperature: float = 0.7,
    model: str | None = None,
    timeout: int = 120,
) -> str | None:
    """
    Call LLM via OpenRouter.
    
    Args:
        messages: List of messages (with role and content)
        temperature: Temperature setting
        model: Model name (default: OPENROUTER_MODEL)
        timeout: Request timeout in seconds
        
    Returns:
        LLM response text or None on error
    """
    if model is None:
        model = OPENROUTER_MODEL
    
    return chat_completion(messages, temperature=temperature, model=model, timeout=timeout)


def call_llm_raw(
    messages: list[dict[str, Any]],
    temperature: float = 0.7,
    model: str | None = None,
    timeout: int = 120,
) -> dict[str, Any] | None:
    """
    Call LLM via OpenRouter and return raw response.
    
    Args:
        messages: List of messages (with role and content)
        temperature: Temperature setting
        model: Model name (default: OPENROUTER_MODEL)
        timeout: Request timeout in seconds
        
    Returns:
        Raw LLM response dict or None on error
    """
    if model is None:
        model = OPENROUTER_MODEL
    
    return chat_completion_raw(messages, temperature=temperature, model=model, timeout=timeout)


async def send_to_ollama(question: str, user_data: dict = None) -> str:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ Ollama API –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏."""
    try:
        temperature = float(user_data.get("ollama_temperature", OLLAMA_TEMPERATURE)) if user_data else OLLAMA_TEMPERATURE
        num_ctx = int(user_data.get("ollama_num_ctx", OLLAMA_NUM_CTX)) if user_data else OLLAMA_NUM_CTX
        num_predict = int(user_data.get("ollama_num_predict", OLLAMA_NUM_PREDICT)) if user_data else OLLAMA_NUM_PREDICT
        system_prompt = user_data.get("ollama_system_prompt", OLLAMA_SYSTEM_PROMPT) if user_data else OLLAMA_SYSTEM_PROMPT
        
        if not (0.0 <= temperature <= 2.0):
            raise ValueError(f"–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0.0 –¥–æ 2.0, –ø–æ–ª—É—á–µ–Ω–æ: {temperature}")
        if num_ctx <= 0 or num_ctx > 32768:
            raise ValueError(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—Ç 1 –¥–æ 32768, –ø–æ–ª—É—á–µ–Ω–æ: {num_ctx}")
        if num_predict <= 0 or num_predict > 8192:
            raise ValueError(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ—Ç 1 –¥–æ 8192, –ø–æ–ª—É—á–µ–Ω–æ: {num_predict}")
        
        api_url = f"{OLLAMA_BASE_URL}/api/chat"
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        enhanced_question = question
        if any(phrase in question.lower() for phrase in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "–æ–±—ä—è—Å–Ω–∏", "—Ä–∞—Å—Å–∫–∞–∂–∏", "–ø–∞—Ä–∞–¥–æ–∫—Å", "–≥–∏–ø–æ—Ç–µ–∑–∞"]):
            enhanced_question = f"{question}\n\n–í–∞–∂–Ω–æ: –æ—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–∞–∫—Ç–∞—Ö. –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º."
        messages.append({"role": "user", "content": enhanced_question})
        
        payload = {
            "model": OLLAMA_MODEL,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_ctx": num_ctx,
                "num_predict": num_predict
            }
        }
        
        logger.info(f"Sending request to Ollama: {api_url}, model: {OLLAMA_MODEL}, temperature: {temperature}, num_ctx: {num_ctx}, num_predict: {num_predict}")
        logger.debug(f"Ollama payload: {payload}")
        
        response = requests.post(api_url, json=payload, timeout=OLLAMA_TIMEOUT)
        
        logger.debug(f"Ollama response status: {response.status_code}")
        response.raise_for_status()
        
        data = response.json()
        
        if "error" in data:
            error_msg = data.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")
            logger.error(f"Ollama API error: {error_msg}, full response: {data}")
            raise ValueError(f"–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏: {error_msg}")
        
        if "message" in data and "content" in data["message"]:
            answer = data["message"]["content"].strip()
            if answer:
                logger.info(f"Ollama response received, length: {len(answer)}")
                return answer
            else:
                logger.warning(f"Ollama returned empty content, full response: {data}")
                raise ValueError("–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
        else:
            logger.warning(f"Unexpected Ollama response structure: {data}")
            raise ValueError("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏")
            
    except requests.exceptions.Timeout:
        logger.exception("Ollama request timeout")
        raise ConnectionError("–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (—Ç–∞–π–º–∞—É—Ç)")
    except requests.exceptions.ConnectionError:
        logger.exception("Ollama connection error")
        raise ConnectionError("–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (–æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è)")
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code if hasattr(e, 'response') and e.response else 'unknown'
        error_body = ""
        if hasattr(e, 'response') and e.response:
            try:
                error_body = e.response.text
                logger.error(f"Ollama HTTP error {status_code}: {error_body}")
            except:
                pass
        logger.exception(f"Ollama HTTP error: {status_code}")
        raise ConnectionError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (HTTP {status_code})")
    except ValueError as e:
        logger.error(f"Ollama model error: {str(e)}")
        raise
    except Exception as e:
        logger.exception(f"Unexpected error in send_to_ollama: {type(e).__name__}: {str(e)}")
        raise ConnectionError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {str(e)}")


async def send_to_ollama_analyze(json_content: str, question: str) -> str:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ Ollama API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ JSON –¥–∞–Ω–Ω—ã—Ö –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏."""
    try:
        api_url = f"{OLLAMA_BASE_URL}/api/chat"
        
        system_prompt = "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ JSON –¥–∞–Ω–Ω—ã–µ –∏ –æ—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ, –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"JSON –¥–∞–Ω–Ω—ã–µ:\n{json_content}\n\n–í–æ–ø—Ä–æ—Å: {question}"}
        ]
        
        payload = {
            "model": ANALYZE_MODEL,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": OLLAMA_TEMPERATURE,
                "num_ctx": OLLAMA_NUM_CTX,
                "num_predict": OLLAMA_NUM_PREDICT
            }
        }
        
        logger.info(f"Sending analyze request to Ollama: {api_url}, model: {ANALYZE_MODEL}")
        logger.debug(f"Ollama analyze payload: {payload}")
        
        response = requests.post(api_url, json=payload, timeout=OLLAMA_TIMEOUT)
        
        logger.debug(f"Ollama analyze response status: {response.status_code}")
        response.raise_for_status()
        
        data = response.json()
        
        if "error" in data:
            error_msg = data.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")
            logger.error(f"Ollama API error: {error_msg}, full response: {data}")
            raise ValueError(f"–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏: {error_msg}")
        
        if "message" in data and "content" in data["message"]:
            answer = data["message"]["content"].strip()
            if answer:
                logger.info(f"Ollama analyze response received, length: {len(answer)}")
                return answer
            else:
                logger.warning(f"Ollama returned empty content, full response: {data}")
                raise ValueError("–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
        else:
            logger.warning(f"Unexpected Ollama response structure: {data}")
            raise ValueError("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏")
            
    except requests.exceptions.Timeout:
        logger.exception("Ollama analyze request timeout")
        raise ConnectionError("–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (—Ç–∞–π–º–∞—É—Ç)")
    except requests.exceptions.ConnectionError:
        logger.exception("Ollama analyze connection error")
        raise ConnectionError("–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (–æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è)")
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code if hasattr(e, 'response') and e.response else 'unknown'
        error_body = ""
        if hasattr(e, 'response') and e.response:
            try:
                error_body = e.response.text
                logger.error(f"Ollama HTTP error {status_code}: {error_body}")
            except:
                pass
        logger.exception(f"Ollama HTTP error: {status_code}")
        raise ConnectionError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (HTTP {status_code})")
    except ValueError as e:
        logger.error(f"Ollama model error: {str(e)}")
        raise
    except Exception as e:
        logger.exception(f"Unexpected error in send_to_ollama_analyze: {type(e).__name__}: {str(e)}")
        raise ConnectionError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {str(e)}")


def get_ollama_settings_display(user_data: dict = None) -> str:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫—É —Å —Ç–µ–∫—É—â–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –º–æ–¥–µ–ª–∏."""
    temperature = float(user_data.get("ollama_temperature", OLLAMA_TEMPERATURE)) if user_data else OLLAMA_TEMPERATURE
    num_ctx = int(user_data.get("ollama_num_ctx", OLLAMA_NUM_CTX)) if user_data else OLLAMA_NUM_CTX
    num_predict = int(user_data.get("ollama_num_predict", OLLAMA_NUM_PREDICT)) if user_data else OLLAMA_NUM_PREDICT
    system_prompt = user_data.get("ollama_system_prompt", OLLAMA_SYSTEM_PROMPT) if user_data else OLLAMA_SYSTEM_PROMPT
    
    return (
        f"üìä –¢–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏:\n"
        f"‚Ä¢ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {temperature}\n"
        f"‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ: {num_ctx}\n"
        f"‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {num_predict}\n"
        f"‚Ä¢ –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {system_prompt[:50]}{'...' if len(system_prompt) > 50 else ''}"
    )
